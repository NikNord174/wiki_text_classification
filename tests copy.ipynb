{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolai/Embedika/wiki_text_classification/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Callable, Union, Tuple\n",
    "from pymystem3 import Mystem\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nikolai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  6336 \n",
      "\n",
      "Categories:\n",
      "{'промышленность', 'игры', 'история', 'литература', 'искусства', 'технологии', 'политика', 'география', 'достопримечательности', 'здравоохранение', 'инфраструктура', 'физика', 'математика'}\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/nikolai/Downloads/mini_wiki_cats.jsonl(1)', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "print('Length of data: ',len(json_list), '\\n')\n",
    "#n = 0\n",
    "categories = []#{'история'}\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    categories.append(result['cats'][0])\n",
    "categories = set(categories)\n",
    "print('Categories:')\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = [[json.loads(json_str).get('text'), json.loads(json_str).get('cats')[0]] for json_str in json_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in categories:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'промышленность': 83,\n",
       " 'игры': 760,\n",
       " 'история': 3127,\n",
       " 'литература': 301,\n",
       " 'искусства': 76,\n",
       " 'технологии': 277,\n",
       " 'политика': 145,\n",
       " 'география': 898,\n",
       " 'достопримечательности': 354,\n",
       " 'здравоохранение': 12,\n",
       " 'инфраструктура': 22,\n",
       " 'физика': 261,\n",
       " 'математика': 20}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats_dict = {cat: 0 for cat in categories}\n",
    "for json_str in json_list:\n",
    "    result = json.loads(json_str)\n",
    "    cats_dict[result['cats'][0]] += 1\n",
    "print('Number of elements in categories:')\n",
    "cats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(tokenizer=None)\n",
    "mystem = Mystem()\n",
    "def tokenize(text, tokenizer=tokenizer):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    clean_text = re.sub(r'[^\\sА-Яа-я]', '', clean_text)\n",
    "    clean_text = clean_text.lower()\n",
    "    tokenized_text = tokenizer(clean_text)\n",
    "    lemmatized_text = [mystem.lemmatize(token)[0] for token in tokenized_text]\n",
    "    clean_doc = [re.sub(r'\\b[0-9]+\\b', '<NUM>', token) for token in lemmatized_text]\n",
    "    clean_doc = [token for token in clean_doc if token not in russian_stopwords]\n",
    "    return clean_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_corpus = [tokenize(article[0], tokenizer) for article in wiki_data]\n",
    "tokenised_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, dict):\n",
    "    \"\"\"Make vector with ones and zeros like [1., 1., 0., 0., 1.]\"\"\"\n",
    "    vec = torch.zeros(len(dict))\n",
    "    for word in sentence:\n",
    "        vec[dict.token2id[word]] += 1\n",
    "    return vec.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki_Dataset_BoW(Dataset):\n",
    "    def __init__(self, data: List = wiki_data) -> None:\n",
    "        tokenizer = get_tokenizer(tokenizer=None)\n",
    "        self.corpus = [tokenize(article[0], tokenizer) for article in data]\n",
    "        great_dictionary = corpora.Dictionary(self.corpus)\n",
    "        self.bow_corpus = [make_bow_vector(doc, great_dictionary) for doc in self.corpus]\n",
    "        cats_dict = {cat: i for cat, i in zip(categories, range(len(categories)))}\n",
    "        self.labels = [cats_dict.get(article[1]) for article in data]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i) -> Union[List, List, int]:\n",
    "        return (\n",
    "            self.bow_corpus[i],\n",
    "            self.labels[i],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki_Dataset_BoW_Vector(Dataset):\n",
    "    def __init__(self, data: List) -> None:\n",
    "        tokenizer = get_tokenizer(tokenizer=None)\n",
    "        self.corpus = [tokenize(article[0], tokenizer) for article in data]\n",
    "        great_dictionary = corpora.Dictionary(self.corpus)\n",
    "        self.bow_corpus = [great_dictionary.doc2bow(doc) for doc in self.corpus]\n",
    "        cats_dict = {cat: i for cat, i in zip(categories, range(len(categories)))}\n",
    "        self.labels = [cats_dict.get(article[1]) for article in cats_dict]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i) -> Union[List, List, int]:\n",
    "        return (\n",
    "            self.corpus[i],\n",
    "            self.bow_corpus[i][0],\n",
    "            self.labels[i],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bow = Wiki_Dataset_BoW(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 1., 1.,  ..., 0., 0., 0.])\n",
      "2\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "2\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "2\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for bow, label in dataset_bow:\n",
    "    print(bow)\n",
    "    print(label)\n",
    "    n += 1\n",
    "    if n > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_test(corpus: Dataset, valid_ratio: float = 0.1,\n",
    "                           test_ratio: float = 0.1):\n",
    "    \"\"\"Split dataset into train, validation, and test.\"\"\"\n",
    "    test_length = int(len(corpus) * test_ratio)\n",
    "    valid_length = int(len(corpus) * valid_ratio)\n",
    "    train_length = len(corpus) - valid_length - test_length\n",
    "    return random_split(\n",
    "        corpus, lengths=[train_length, valid_length, test_length],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 250\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = split_train_valid_test(\n",
    "    dataset_bow, valid_ratio=0.1, test_ratio=0.1\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)#, collate_fn=lambda x: x )\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)#, collate_fn=lambda x: x )\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)#, collate_fn=lambda x: x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 129435])\n",
      "torch.Size([250])\n"
     ]
    }
   ],
   "source": [
    "for vectors, labels in train_loader:\n",
    "    print(vectors.size())\n",
    "    print(labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 129435,\n",
    "        num_labels: int = 13\n",
    "    ) -> None:\n",
    "        super(Linear_Classifier, self).__init__()\n",
    "        fc1 = nn.Linear(vocab_size, vocab_size//10)\n",
    "        fc2 = nn.Linear(vocab_size//10, vocab_size//100)\n",
    "        fc3 = nn.Linear(vocab_size//100, vocab_size//1000)\n",
    "        fc4 = nn.Linear(vocab_size//1000, num_labels)\n",
    "        leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.model = nn.Sequential(\n",
    "            fc1, leaky_relu, fc2, leaky_relu, fc3, leaky_relu, fc4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear_Classifier(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=129435, out_features=12943, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Linear(in_features=12943, out_features=1294, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Linear(in_features=1294, out_features=129, bias=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Linear(in_features=129, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Linear_Classifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred: torch.Tensor,\n",
    "             ground: torch.Tensor) -> float:\n",
    "    pred_class = pred.argmax(dim=1)\n",
    "    return sum(pred_class==ground).item()\n",
    "\n",
    "\n",
    "def train(model: nn.Sequential,\n",
    "          device: torch.device,\n",
    "          train_loader: DataLoader,\n",
    "          optimizer,\n",
    "          criterion) -> None:\n",
    "    model.train()\n",
    "    for bow_vectors, labels in train_loader:\n",
    "        bow_vectors = bow_vectors.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(bow_vectors)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(model: nn.Sequential,\n",
    "         device: torch.device,\n",
    "         test_loader: DataLoader,\n",
    "         criterion) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for bow_vectors, labels in test_loader:\n",
    "            bow_vectors = bow_vectors.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(bow_vectors)\n",
    "            acc += accuracy(output, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.item()\n",
    "    return (test_loss / len(test_loader.dataset),\n",
    "            acc / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000048?line=7'>8</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000048?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000048?line=9'>10</a>\u001b[0m     train(model, device, train_loader, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000048?line=10'>11</a>\u001b[0m     test_loss, acc \u001b[39m=\u001b[39m test(model, device, test_loader, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000048?line=11'>12</a>\u001b[0m     acc_list\u001b[39m.\u001b[39mappend(acc)\n",
      "\u001b[1;32m/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb Cell 11'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000016?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000016?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nikolai/Embedika/wiki_text_classification/tests.ipynb#ch0000016?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/Embedika/wiki_text_classification/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Embedika/wiki_text_classification/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Embedika/wiki_text_classification/venv/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Embedika/wiki_text_classification/venv/lib/python3.8/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/Embedika/wiki_text_classification/venv/lib/python3.8/site-packages/torch/optim/adam.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    264\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 265\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[1;32m    268\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "t0 = time.time()\n",
    "acc_list =[]\n",
    "test_loss_list = []\n",
    "epochs = 1\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "for epoch in range(epochs):\n",
    "    train(model, device, train_loader, optimizer, criterion)\n",
    "    test_loss, acc = test(model, device, test_loader, criterion)\n",
    "    acc_list.append(acc)\n",
    "    test_loss_list.append(test_loss)\n",
    "    t1 = (time.time() - t0) / 60\n",
    "    print('Epoch: {}, test loss: {:.3f}, accuracy: {:.3f}, ' + \n",
    "            'time: {:.2f} min'.format(epoch+1, test_loss, acc, t1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3921a8f4ef7b812cda52bece9756bc0377626aba568a8c633c36ad41643f6286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
